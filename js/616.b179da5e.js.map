{"version":3,"file":"js/616.b179da5e.js","mappings":"uNAIA,MAAMA,EAASC,EAAQ,OAchBC,eAAeC,GAAS,OAC3BC,EAAM,QACNC,EAAO,MACPC,EAAK,WACLC,EAAU,OACVC,EAAM,UACNC,EAAS,QACTC,EAAO,QACPC,IAEA,MAAM,QAAEC,GAAYC,EAAAA,EAAMC,OACpB,aAAEC,GAAiBH,GACnB,KAAEI,EAAI,SAAEC,EAAQ,QAAEC,EAAO,QAAEC,GAAYJ,EAEvCK,EAAS,IAAIpB,EAAO,CACtBqB,OAAQH,EACRI,QAASL,EACTM,YAAa,EACbC,yBAAyB,IAG7B,IACI,MAAMC,QAAeL,EAAOM,KAAKC,YAAYC,OAAO,IAC7CC,EAAWV,EAASf,EAAQC,EAASW,EAAMV,GAAO,GACrDmB,QAAQ,IAGZjB,MAEA,UAAW,MAAMsB,KAASL,EAAQ,CAC9B,MAAMM,EAAUD,GAAS,CAAC,EAC1BrB,IAAYsB,EAChB,CAEArB,KACJ,CAAE,MAAOsB,GAEL,MADArB,IAAUqB,GACJA,CACV,CACJ,CAWA,SAASH,EAAWI,EAAe7B,EAAQC,EAASW,EAAMV,EAAO4B,GAC7D,IAAIC,EAAO,CAAC,EACZ,OAAQnB,GAEJ,IAAK,MACDmB,EAAO,CACHC,MAAOH,EACPI,SAAUC,EAAgBlC,EAAQC,IAEtC,MAER,OAAO8B,CACX,CAOA,SAASG,EAAgBlC,EAAQC,GAC7B,SAASkC,EAAWlC,GAChB,MAAMmC,EAAQ,GAEd,IAAK,IAAIC,EAAI,EAAGA,EAAIpC,EAAQqC,OAAS,EAAGD,IAAK,CACzC,MAAMf,EAAOrB,EAAQoC,GACrBD,EAAMG,KAAK,CACP,KAAQ,OACR,QAAWjB,EAAKkB,QAEpBJ,EAAMG,KAAK,CACP,KAAQ,YACR,QAAWjB,EAAKmB,QAExB,CACA,OAAOL,CACX,CACA,IAAIM,EAAMP,EAAWlC,GAKrB,OAJAyC,EAAIH,KAAK,CACL,KAAQ,OACR,QAAWvC,IAER0C,CACX,CAEO,SAASC,EAAYC,GACxB,GAAc,WAAVA,EACA,MAAO,CAAEjB,QAAS,IAGtB,MAAMkB,EAAOD,EAAME,QAAQ,GAE3B,MAA2B,SAAvBD,EAAKE,cACE,CAAEpB,QAAS,IAGlBkB,EAAKG,MAAMC,kBACJ,CAAEA,kBAAmBJ,EAAKG,MAAMC,mBAEhC,CAAEtB,QAASkB,EAAKG,MAAMrB,QAErC,C","sources":["webpack://THChatUI/./src/api/Azure/index.js"],"sourcesContent":["/**\n * @fileoverview Azure平台的HTTP调用。\n * 接口采用SSE请求方式，不需要跨域配置\n */\nconst OpenAI = require('openai')\nimport store from '../../store';\n\n/**\n * 调用Azure平台的接口\n * @param {string} prompt - 用户输入的问题\n * @param {Array} history - 历史对话消息\n * @param {Array} files - 文件列表\n * @param {AbortController} controller - 控制请求的取消\n * @param {Function} onopen - 连接成功回调\n * @param {Function} onmessage - 接收消息回调\n * @param {Function} onclose - 连接关闭回调\n * @param {Function} onerror - 错误处理回调\n */\nexport async function fetchAPI({\n    prompt,\n    history,\n    files,\n    controller,\n    onopen,\n    onmessage,\n    onclose,\n    onerror\n}) {\n    const { setting } = store.state;\n    const { model_config } = setting;\n    const { type, endpoint, api_key, version } = model_config;\n\n    const openai = new OpenAI({\n        apiKey: api_key,\n        baseURL: endpoint,\n        temperature: 0,\n        dangerouslyAllowBrowser: true\n    });\n\n    try {\n        const stream = await openai.chat.completions.create({\n            ...preProcess(version, prompt, history, type, files, false),\n            stream: true,\n        });\n\n        onopen?.();\n\n        for await (const chunk of stream) {\n            const content = chunk || {};\n            onmessage?.(content);\n        }\n\n        onclose?.();\n    } catch (error) {\n        onerror?.(error);\n        throw error;\n    }\n}\n\n/**\n * 构建请求体\n * @param {string} model_version - 模型版本标识\n * @param {string} prompt - 用户当前的输入内容\n * @param {Array} history - 历史对话记录数组\n * @param {string} type - 模型类型标识\n * @param {Array} files - 上传的文件数组，主要包含图片的base64数据\n * @param {boolean} is_search - 是否启用网络搜索功能\n */\nfunction preProcess(model_version, prompt, history, type, files, is_search) {\n    let body = {};\n    switch (type) {\n        // 文本输入格式\n        case \"llm\":\n            body = {\n                model: model_version,\n                messages: buildLLMMessage(prompt, history)\n            }\n            break;\n    }\n    return body;\n}\n\n/**\n * 构建LLM文本对话消息\n * @param {string} prompt - 用户当前的输入内容\n * @param {Array} history - 历史对话记录数组\n */\nfunction buildLLMMessage(prompt, history) {\n    function getHistory(history) {\n        const array = [];\n        // 排除最后一条 history，因为是本次刚发的消息\n        for (let i = 0; i < history.length - 1; i++) {\n            const chat = history[i];\n            array.push({\n                \"role\": \"user\",\n                \"content\": chat.query\n            });\n            array.push({\n                \"role\": \"assistant\",\n                \"content\": chat.answer\n            });\n        }\n        return array;\n    }\n    let arr = getHistory(history)\n    arr.push({\n        \"role\": \"user\",\n        \"content\": prompt\n    })\n    return arr;\n}\n\nexport function postProcess(event) {\n    if (event === '[DONE]') {\n        return { content: '' };\n    }\n\n    const data = event.choices[0];\n\n    if (data.finish_reason === \"stop\") {\n        return { content: '' };\n    }\n\n    if (data.delta.reasoning_content) {\n        return { reasoning_content: data.delta.reasoning_content };\n    } else {\n        return { content: data.delta.content };\n    }\n}\n"],"names":["OpenAI","require","async","fetchAPI","prompt","history","files","controller","onopen","onmessage","onclose","onerror","setting","store","state","model_config","type","endpoint","api_key","version","openai","apiKey","baseURL","temperature","dangerouslyAllowBrowser","stream","chat","completions","create","preProcess","chunk","content","error","model_version","is_search","body","model","messages","buildLLMMessage","getHistory","array","i","length","push","query","answer","arr","postProcess","event","data","choices","finish_reason","delta","reasoning_content"],"sourceRoot":""}